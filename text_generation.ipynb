{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading packages and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "41qhwIFO0OSG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KBVNVajo0OSR"
      },
      "outputs": [],
      "source": [
        "with open('metamorphosis_clean.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjJMhYlG0OSS",
        "outputId": "268b72f1-bc16-4d3f-f807-f286fb6d9a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "himself transformed in his bed into a horrible vermin. He lay on his\n",
            "armour-like back, and if he lifted his head a little he could see his\n",
            "brown belly, slightly domed and divided by arches into stiff sections.\n",
            "The bedding was hardly able to cover it and seemed ready to slide off\n",
            "any moment. His many legs, pitifully thin compared with the size of the\n",
            "rest of him, waved about helplessly as he looked.\n",
            "\n",
            "“What’s happened to me?” he thought. It wasn’t a dream. His room, a\n",
            "proper human room although a little too small, lay peacefully \n"
          ]
        }
      ],
      "source": [
        "print(text[:600])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49i3mBTK0OST",
        "outputId": "8467e5c8-372f-44ef-ae81-4c064a84c59f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "118412"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FPebq6X70OSV"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the text (word to vectors and vectors to word)\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch:i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i:ch for i, ch in enumerate(chars)}\n",
        "no_of_vocab = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slj6f2S90OSV",
        "outputId": "6599c5e8-4112-4524-fe34-62d41f932ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ç', '—', '’', '“', '”']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(chars)\n",
        "no_of_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMGgfO50OSW",
        "outputId": "9abe0ac6-269b-41b0-b09f-2d424638f631"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(118341, 71)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the maximum sequence length (max_len) to be the length of the longest sequence\n",
        "\n",
        "max_len = len(max(text.split('\\n'), key=len))\n",
        "# max_len = 40\n",
        "\n",
        "# Creating training data vectors and label vectors\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(0, len(text)-max_len, 1):\n",
        "    X.append([char_to_int[ch] for ch in text[i:i+max_len]])\n",
        "    y.append(char_to_int[text[i+max_len]])\n",
        "no_of_patterns = len(X)\n",
        "\n",
        "no_of_patterns, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vkY_64G60OSX"
      },
      "outputs": [],
      "source": [
        "# Padding the training sequences to the same length\n",
        "X = pad_sequences(X, maxlen=max_len, padding='post')\n",
        "\n",
        "# Converting labels to categorical format\n",
        "y = ku.to_categorical(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mkwpcK50OSY",
        "outputId": "452f4335-2e70-4f87-f11d-382ee3d60a9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([24, 46, 37,  1, 45, 47, 50, 46, 41, 46, 39,  5,  1, 55, 40, 37, 46,\n",
              "         1, 17, 50, 37, 39, 47, 50,  1, 27, 33, 45, 51, 33,  1, 55, 47, 43,\n",
              "        37,  1, 38, 50, 47, 45,  1, 52, 50, 47, 53, 34, 44, 37, 36,  1, 36,\n",
              "        50, 37, 33, 45, 51,  5,  1, 40, 37,  1, 38, 47, 53, 46, 36,  0, 40,\n",
              "        41, 45, 51], dtype=int32),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0], y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdhzPAiv0OSZ",
        "outputId": "483df52b-a73b-4e31-95ef-51d206c4bacb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(118341, 71, 118341, 64)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Checking the shape of the training data and label vectors\n",
        "X.shape[0], X.shape[1], y.shape[0], y.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4hDC-Tyr0OSb"
      },
      "outputs": [],
      "source": [
        "# Defining the model architecture\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=no_of_vocab, output_dim=64))\n",
        "model.add(LSTM(units=256, return_sequences=True))              # , activation='tanh', recurrent_activation='sigmoid', return_sequences=True\n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(units=no_of_vocab, activation='softmax'))\n",
        "optim = Adam(learning_rate=1e-3)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71iP2n680OSc",
        "outputId": "6c709fbf-252a-403c-a2dc-ffc7ee04c890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          4096      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 256)         328704    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               197120    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 538176 (2.05 MB)\n",
            "Trainable params: 538176 (2.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktap0mBa0OSc",
        "outputId": "bab52ac9-de6c-45c7-e400-e35bb84ce377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-23 01:29:06.645775: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
            "2024-01-23 01:29:07.015305: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f495c00db00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-01-23 01:29:07.015382: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Quadro M1200, Compute Capability 5.0\n",
            "2024-01-23 01:29:07.048264: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1705998547.210125    4616 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1850/1850 [==============================] - 92s 45ms/step - loss: 2.3177 - accuracy: 0.3460\n",
            "Epoch 2/100\n",
            "1850/1850 [==============================] - 80s 43ms/step - loss: 1.8304 - accuracy: 0.4624\n",
            "Epoch 3/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 1.8975 - accuracy: 0.4461\n",
            "Epoch 4/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 1.6955 - accuracy: 0.4955\n",
            "Epoch 5/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.5612 - accuracy: 0.5331\n",
            "Epoch 6/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 1.4804 - accuracy: 0.5545\n",
            "Epoch 7/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 1.4549 - accuracy: 0.5611\n",
            "Epoch 8/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.3922 - accuracy: 0.5760\n",
            "Epoch 9/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 1.3422 - accuracy: 0.5908\n",
            "Epoch 10/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.2986 - accuracy: 0.6024\n",
            "Epoch 11/100\n",
            "1850/1850 [==============================] - 80s 43ms/step - loss: 1.2718 - accuracy: 0.6099\n",
            "Epoch 12/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.2492 - accuracy: 0.6150\n",
            "Epoch 13/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.2114 - accuracy: 0.6260\n",
            "Epoch 14/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.1760 - accuracy: 0.6345\n",
            "Epoch 15/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.1400 - accuracy: 0.6452\n",
            "Epoch 16/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.1076 - accuracy: 0.6547\n",
            "Epoch 17/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.1082 - accuracy: 0.6549\n",
            "Epoch 18/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 1.1476 - accuracy: 0.6460\n",
            "Epoch 19/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.0898 - accuracy: 0.6604\n",
            "Epoch 20/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.0395 - accuracy: 0.6755\n",
            "Epoch 21/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.0140 - accuracy: 0.6820\n",
            "Epoch 22/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 0.9794 - accuracy: 0.6926\n",
            "Epoch 23/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9627 - accuracy: 0.6991\n",
            "Epoch 24/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9242 - accuracy: 0.7087\n",
            "Epoch 25/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9333 - accuracy: 0.7066\n",
            "Epoch 26/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9285 - accuracy: 0.7103\n",
            "Epoch 27/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 0.8774 - accuracy: 0.7250\n",
            "Epoch 28/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 0.8797 - accuracy: 0.7232\n",
            "Epoch 29/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 0.9947 - accuracy: 0.6939\n",
            "Epoch 30/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9909 - accuracy: 0.6944\n",
            "Epoch 31/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9496 - accuracy: 0.7048\n",
            "Epoch 32/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9002 - accuracy: 0.7176\n",
            "Epoch 33/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9277 - accuracy: 0.7103\n",
            "Epoch 34/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.0833 - accuracy: 0.6681\n",
            "Epoch 35/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 1.0753 - accuracy: 0.6700\n",
            "Epoch 36/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.1201 - accuracy: 0.6558\n",
            "Epoch 37/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 1.0899 - accuracy: 0.6636\n",
            "Epoch 38/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.0685 - accuracy: 0.6665\n",
            "Epoch 39/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.0469 - accuracy: 0.6738\n",
            "Epoch 40/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.0269 - accuracy: 0.6788\n",
            "Epoch 41/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 1.0036 - accuracy: 0.6835\n",
            "Epoch 42/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 0.9796 - accuracy: 0.6904\n",
            "Epoch 43/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9551 - accuracy: 0.6974\n",
            "Epoch 44/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9320 - accuracy: 0.7040\n",
            "Epoch 45/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.9101 - accuracy: 0.7113\n",
            "Epoch 46/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.8886 - accuracy: 0.7179\n",
            "Epoch 47/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.8657 - accuracy: 0.7254\n",
            "Epoch 48/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.8463 - accuracy: 0.7313\n",
            "Epoch 49/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.8272 - accuracy: 0.7363\n",
            "Epoch 50/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.8057 - accuracy: 0.7432\n",
            "Epoch 51/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.7867 - accuracy: 0.7484\n",
            "Epoch 52/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.7685 - accuracy: 0.7550\n",
            "Epoch 53/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.7509 - accuracy: 0.7596\n",
            "Epoch 54/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.7323 - accuracy: 0.7660\n",
            "Epoch 55/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.7143 - accuracy: 0.7714\n",
            "Epoch 56/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.6967 - accuracy: 0.7766\n",
            "Epoch 57/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 0.6809 - accuracy: 0.7822\n",
            "Epoch 58/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.6615 - accuracy: 0.7882\n",
            "Epoch 59/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.6488 - accuracy: 0.7929\n",
            "Epoch 60/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.6334 - accuracy: 0.7970\n",
            "Epoch 61/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.6179 - accuracy: 0.8027\n",
            "Epoch 62/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.6024 - accuracy: 0.8069\n",
            "Epoch 63/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.5873 - accuracy: 0.8119\n",
            "Epoch 64/100\n",
            "1850/1850 [==============================] - 77s 42ms/step - loss: 0.5765 - accuracy: 0.8159\n",
            "Epoch 65/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 0.5639 - accuracy: 0.8197\n",
            "Epoch 66/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.5499 - accuracy: 0.8232\n",
            "Epoch 67/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.5360 - accuracy: 0.8276\n",
            "Epoch 68/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.5261 - accuracy: 0.8315\n",
            "Epoch 69/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.5114 - accuracy: 0.8369\n",
            "Epoch 70/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.5016 - accuracy: 0.8390\n",
            "Epoch 71/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4886 - accuracy: 0.8422\n",
            "Epoch 72/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4778 - accuracy: 0.8466\n",
            "Epoch 73/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4715 - accuracy: 0.8488\n",
            "Epoch 74/100\n",
            "1850/1850 [==============================] - 79s 43ms/step - loss: 0.4598 - accuracy: 0.8522\n",
            "Epoch 75/100\n",
            "1850/1850 [==============================] - 81s 44ms/step - loss: 0.4544 - accuracy: 0.8534\n",
            "Epoch 76/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4391 - accuracy: 0.8597\n",
            "Epoch 77/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4345 - accuracy: 0.8606\n",
            "Epoch 78/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4273 - accuracy: 0.8617\n",
            "Epoch 79/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4196 - accuracy: 0.8633\n",
            "Epoch 80/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4102 - accuracy: 0.8678\n",
            "Epoch 81/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.4033 - accuracy: 0.8701\n",
            "Epoch 82/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3986 - accuracy: 0.8698\n",
            "Epoch 83/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3941 - accuracy: 0.8721\n",
            "Epoch 84/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3813 - accuracy: 0.8764\n",
            "Epoch 85/100\n",
            "1850/1850 [==============================] - 77s 42ms/step - loss: 0.3725 - accuracy: 0.8793\n",
            "Epoch 86/100\n",
            "1850/1850 [==============================] - 77s 42ms/step - loss: 0.3735 - accuracy: 0.8787\n",
            "Epoch 87/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3601 - accuracy: 0.8833\n",
            "Epoch 88/100\n",
            "1850/1850 [==============================] - 77s 42ms/step - loss: 0.3631 - accuracy: 0.8816\n",
            "Epoch 89/100\n",
            "1850/1850 [==============================] - 77s 42ms/step - loss: 0.3513 - accuracy: 0.8868\n",
            "Epoch 90/100\n",
            "1850/1850 [==============================] - 77s 41ms/step - loss: 0.3483 - accuracy: 0.8870\n",
            "Epoch 91/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3459 - accuracy: 0.8881\n",
            "Epoch 92/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3337 - accuracy: 0.8913\n",
            "Epoch 93/100\n",
            "1850/1850 [==============================] - 80s 43ms/step - loss: 0.3382 - accuracy: 0.8896\n",
            "Epoch 94/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3314 - accuracy: 0.8916\n",
            "Epoch 95/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3230 - accuracy: 0.8951\n",
            "Epoch 96/100\n",
            "1850/1850 [==============================] - 79s 42ms/step - loss: 0.3263 - accuracy: 0.8926\n",
            "Epoch 97/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3218 - accuracy: 0.8949\n",
            "Epoch 98/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3185 - accuracy: 0.8955\n",
            "Epoch 99/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3089 - accuracy: 0.9000\n",
            "Epoch 100/100\n",
            "1850/1850 [==============================] - 78s 42ms/step - loss: 0.3170 - accuracy: 0.8956\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f49b23af9d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training the model\n",
        "model.fit(X, y, epochs=100, batch_size=64, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlJDBYbA0OSd"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "model.save('model_small.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P6c2NgfO0OSe",
        "outputId": "c2c0536f-37f3-4ac1-d57f-ddb32aa6ddcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          4096      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 256)         328704    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               197120    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 538176 (2.05 MB)\n",
            "Trainable params: 538176 (2.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Loading the model for inference\n",
        "\n",
        "model.load_weights('model_small.h5')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5uJppTun0OSf"
      },
      "outputs": [],
      "source": [
        "# Alternative function to generate text\n",
        "\n",
        "# def generate_text(seed, num_chars):\n",
        "#     # Initialize the generated text\n",
        "#     generated_text = seed\n",
        "#     # Encode the seed as integers\n",
        "#     encoded_seed = [char_to_int[char] for char in seed]\n",
        "#     # Pad the seed\n",
        "#     padded_seed = pad_sequences([encoded_seed], maxlen=max_len, padding='post')\n",
        "#     # Generate characters\n",
        "#     for i in range(num_chars):\n",
        "#         # Get the next character probabilities\n",
        "#         predictions = model.predict(padded_seed, verbose=0)[0]\n",
        "#         # Get the index of the character with the highest probability\n",
        "#         index = np.argmax(predictions)\n",
        "#         # Add the character to the generated text\n",
        "#         generated_text += int_to_char[index]\n",
        "#         # Update the padded seed with the latest character\n",
        "#         padded_seed = np.append(padded_seed[0][1:], index)\n",
        "#         padded_seed = pad_sequences([padded_seed], maxlen=max_len, padding='post')\n",
        "#\n",
        "#     return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rIS2KPjJ0OSf"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "\n",
        "def generate_text(num_chars, lstm_model, random_text=None):\n",
        "    # pick a random seed\n",
        "    random_text = np.random.randint(0, len(X)-1)\n",
        "    pattern = X[random_text]\n",
        "    seed = ''.join([int_to_char[value] for value in pattern])\n",
        "    print(\"Seed:\")\n",
        "    print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "    print(\"Generated text:\\n\")\n",
        "    # Initialize the generated text\n",
        "    generated_text = ''\n",
        "    # Encode the seed as integers\n",
        "    encoded_seed = [char_to_int[char] for char in seed]\n",
        "    # Pad the seed\n",
        "    padded_seed = pad_sequences([encoded_seed], maxlen=max_len, padding='post')\n",
        "    # Generate characters\n",
        "    for _ in range(num_chars):\n",
        "        # Get the next character probabilities\n",
        "        predictions = lstm_model.predict(padded_seed, verbose=0)[0]\n",
        "        # Get the index of the character with the highest probability\n",
        "        index = np.argmax(predictions)\n",
        "        # Add the character to the generated text\n",
        "        generated_text += int_to_char[index]\n",
        "        # Update the padded seed with the latest character\n",
        "        padded_seed = np.append(padded_seed[0][1:], index)\n",
        "        padded_seed = pad_sequences([padded_seed], maxlen=max_len, padding='post')\n",
        "        \n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zNdFLBbB0OSg",
        "outputId": "afd6fb63-db3f-4e6f-fb33-580a9cd4759c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" im. And so he ran up to his father,\n",
            "stopped when his father stopped, sc \"\n",
            "Generated text:\n",
            "\n",
            "urried forwards again when he stretched and she had to\n",
            "deat set aside for exes with his father’s behaviour that she was no\n",
            "pleasure for Gregor’s room as a sign of his pleasure, that was something that they had hoped there was already in tears that they had been standing there.\n",
            "\n",
            "Gregor then there was no immediate notice price without delaying as\n",
            "they landed looked as if he was clearly as he had sending avoiring\n",
            "anything a lack of his body to a state. But his father as if they had\n",
            "become much lighter at his sister was a sill-or whether it was a sill\n",
            "and most of what they were even a long time ag\n"
          ]
        }
      ],
      "source": [
        "# Generate some text with specified length using random seed text from the data\n",
        "\n",
        "# generated_text = generate_text(seed=\"Rejoice\", num_chars=200)\n",
        "generated_text = generate_text(num_chars=600, lstm_model=model)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Larger model architecture (training and inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OlumtgSJ0OSh"
      },
      "outputs": [],
      "source": [
        "# Defining the larger LSTM model architecture\n",
        "\n",
        "model_large = Sequential()\n",
        "model_large.add(Embedding(input_dim=no_of_vocab, output_dim=80))\n",
        "model_large.add(LSTM(1024, return_sequences=True))\n",
        "model_large.add(Dropout(0.2))\n",
        "model_large.add(LSTM(128, return_sequences=True))\n",
        "model_large.add(Dropout(0.2))\n",
        "model_large.add(LSTM(128))\n",
        "model_large.add(Dropout(0.2))\n",
        "model_large.add(Dense(units=no_of_vocab, activation='softmax'))\n",
        "model_large.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "filepath=\"best-model-weights-{epoch:02d}-{loss:.4f}-{accuracy:.2f}.hdf5\"\n",
        "\n",
        "# Defining the checkpoints and callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, min_lr=0.001)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint, reduce_lr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xqLhjk90OSi",
        "outputId": "76b69632-0256-49b4-87c1-74a7e24137c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 80)          5120      \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, None, 1024)        4526080   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 1024)        0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, None, 128)         590336    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 128)         0         \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5261376 (20.07 MB)\n",
            "Trainable params: 5261376 (20.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_large.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKOgW6P60OSi",
        "outputId": "4714452e-1d96-4c1f-cd26-c8ab46a1af97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 2.7980 - accuracy: 0.2222\n",
            "Epoch 1: loss improved from inf to 2.79799, saving model to best-model-weights-01-2.7980-0.22.hdf5\n",
            "1850/1850 [==============================] - 117s 59ms/step - loss: 2.7980 - accuracy: 0.2222 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 2.2182 - accuracy: 0.3652\n",
            "Epoch 2: loss improved from 2.79799 to 2.21816, saving model to best-model-weights-02-2.2182-0.37.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 2.2182 - accuracy: 0.3652 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.9708 - accuracy: 0.4273\n",
            "Epoch 3: loss improved from 2.21816 to 1.97085, saving model to best-model-weights-03-1.9708-0.43.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.9708 - accuracy: 0.4273 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.8092 - accuracy: 0.4660\n",
            "Epoch 4: loss improved from 1.97085 to 1.80923, saving model to best-model-weights-04-1.8092-0.47.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.8092 - accuracy: 0.4660 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.7032 - accuracy: 0.4931\n",
            "Epoch 5: loss improved from 1.80923 to 1.70321, saving model to best-model-weights-05-1.7032-0.49.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.7032 - accuracy: 0.4931 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.6307 - accuracy: 0.5128\n",
            "Epoch 6: loss improved from 1.70321 to 1.63069, saving model to best-model-weights-06-1.6307-0.51.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.6307 - accuracy: 0.5128 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.5776 - accuracy: 0.5270\n",
            "Epoch 7: loss improved from 1.63069 to 1.57761, saving model to best-model-weights-07-1.5776-0.53.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.5776 - accuracy: 0.5270 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.5380\n",
            "Epoch 8: loss improved from 1.57761 to 1.53110, saving model to best-model-weights-08-1.5311-0.54.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.5311 - accuracy: 0.5380 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.4976 - accuracy: 0.5483\n",
            "Epoch 9: loss improved from 1.53110 to 1.49759, saving model to best-model-weights-09-1.4976-0.55.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.4976 - accuracy: 0.5483 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.4677 - accuracy: 0.5558\n",
            "Epoch 10: loss improved from 1.49759 to 1.46764, saving model to best-model-weights-10-1.4676-0.56.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.4676 - accuracy: 0.5558 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.4435 - accuracy: 0.5615\n",
            "Epoch 11: loss improved from 1.46764 to 1.44346, saving model to best-model-weights-11-1.4435-0.56.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.4435 - accuracy: 0.5615 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.4235 - accuracy: 0.5675\n",
            "Epoch 12: loss improved from 1.44346 to 1.42354, saving model to best-model-weights-12-1.4235-0.57.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.4235 - accuracy: 0.5674 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.4015 - accuracy: 0.5761\n",
            "Epoch 13: loss improved from 1.42354 to 1.40149, saving model to best-model-weights-13-1.4015-0.58.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.4015 - accuracy: 0.5761 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.3827 - accuracy: 0.5782\n",
            "Epoch 14: loss improved from 1.40149 to 1.38277, saving model to best-model-weights-14-1.3828-0.58.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.3828 - accuracy: 0.5782 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.3650 - accuracy: 0.5842\n",
            "Epoch 15: loss improved from 1.38277 to 1.36502, saving model to best-model-weights-15-1.3650-0.58.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.3650 - accuracy: 0.5842 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.3479 - accuracy: 0.5889\n",
            "Epoch 16: loss improved from 1.36502 to 1.34794, saving model to best-model-weights-16-1.3479-0.59.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.3479 - accuracy: 0.5889 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.3338 - accuracy: 0.5918\n",
            "Epoch 17: loss improved from 1.34794 to 1.33392, saving model to best-model-weights-17-1.3339-0.59.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.3339 - accuracy: 0.5918 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.3194 - accuracy: 0.5966\n",
            "Epoch 18: loss improved from 1.33392 to 1.31945, saving model to best-model-weights-18-1.3194-0.60.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.3194 - accuracy: 0.5966 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.3047 - accuracy: 0.6010\n",
            "Epoch 19: loss improved from 1.31945 to 1.30467, saving model to best-model-weights-19-1.3047-0.60.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.3047 - accuracy: 0.6010 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.2926 - accuracy: 0.6042\n",
            "Epoch 20: loss improved from 1.30467 to 1.29265, saving model to best-model-weights-20-1.2926-0.60.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2926 - accuracy: 0.6042 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.2819 - accuracy: 0.6059\n",
            "Epoch 21: loss improved from 1.29265 to 1.28191, saving model to best-model-weights-21-1.2819-0.61.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2819 - accuracy: 0.6059 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.2730 - accuracy: 0.6087\n",
            "Epoch 22: loss improved from 1.28191 to 1.27301, saving model to best-model-weights-22-1.2730-0.61.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2730 - accuracy: 0.6087 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.2576 - accuracy: 0.6126\n",
            "Epoch 23: loss improved from 1.27301 to 1.25753, saving model to best-model-weights-23-1.2575-0.61.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2575 - accuracy: 0.6127 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.2473 - accuracy: 0.6150\n",
            "Epoch 24: loss improved from 1.25753 to 1.24726, saving model to best-model-weights-24-1.2473-0.61.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2473 - accuracy: 0.6150 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.2357 - accuracy: 0.6184\n",
            "Epoch 25: loss improved from 1.24726 to 1.23570, saving model to best-model-weights-25-1.2357-0.62.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2357 - accuracy: 0.6184 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.2272 - accuracy: 0.6202\n",
            "Epoch 26: loss improved from 1.23570 to 1.22715, saving model to best-model-weights-26-1.2272-0.62.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.2272 - accuracy: 0.6202 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.2176 - accuracy: 0.6238\n",
            "Epoch 27: loss improved from 1.22715 to 1.21762, saving model to best-model-weights-27-1.2176-0.62.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2176 - accuracy: 0.6238 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.2087 - accuracy: 0.6253\n",
            "Epoch 28: loss improved from 1.21762 to 1.20866, saving model to best-model-weights-28-1.2087-0.63.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.2087 - accuracy: 0.6253 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1979 - accuracy: 0.6286\n",
            "Epoch 29: loss improved from 1.20866 to 1.19789, saving model to best-model-weights-29-1.1979-0.63.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1979 - accuracy: 0.6286 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1918 - accuracy: 0.6289\n",
            "Epoch 30: loss improved from 1.19789 to 1.19183, saving model to best-model-weights-30-1.1918-0.63.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1918 - accuracy: 0.6289 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1801 - accuracy: 0.6350\n",
            "Epoch 31: loss improved from 1.19183 to 1.18008, saving model to best-model-weights-31-1.1801-0.64.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1801 - accuracy: 0.6351 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1754 - accuracy: 0.6352\n",
            "Epoch 32: loss improved from 1.18008 to 1.17540, saving model to best-model-weights-32-1.1754-0.64.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1754 - accuracy: 0.6352 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1636 - accuracy: 0.6385\n",
            "Epoch 33: loss improved from 1.17540 to 1.16362, saving model to best-model-weights-33-1.1636-0.64.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1636 - accuracy: 0.6385 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1578 - accuracy: 0.6394\n",
            "Epoch 34: loss improved from 1.16362 to 1.15774, saving model to best-model-weights-34-1.1577-0.64.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1577 - accuracy: 0.6394 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1566 - accuracy: 0.6403\n",
            "Epoch 35: loss improved from 1.15774 to 1.15665, saving model to best-model-weights-35-1.1567-0.64.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1567 - accuracy: 0.6403 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1491 - accuracy: 0.6434\n",
            "Epoch 36: loss improved from 1.15665 to 1.14914, saving model to best-model-weights-36-1.1491-0.64.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1491 - accuracy: 0.6434 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1360 - accuracy: 0.6464\n",
            "Epoch 37: loss improved from 1.14914 to 1.13601, saving model to best-model-weights-37-1.1360-0.65.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1360 - accuracy: 0.6464 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1311 - accuracy: 0.6481\n",
            "Epoch 38: loss improved from 1.13601 to 1.13109, saving model to best-model-weights-38-1.1311-0.65.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1311 - accuracy: 0.6481 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1235 - accuracy: 0.6498\n",
            "Epoch 39: loss improved from 1.13109 to 1.12349, saving model to best-model-weights-39-1.1235-0.65.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1235 - accuracy: 0.6498 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1136 - accuracy: 0.6524\n",
            "Epoch 40: loss improved from 1.12349 to 1.11366, saving model to best-model-weights-40-1.1137-0.65.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.1137 - accuracy: 0.6524 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1071 - accuracy: 0.6554\n",
            "Epoch 41: loss improved from 1.11366 to 1.10703, saving model to best-model-weights-41-1.1070-0.66.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.1070 - accuracy: 0.6555 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.1046 - accuracy: 0.6543\n",
            "Epoch 42: loss improved from 1.10703 to 1.10457, saving model to best-model-weights-42-1.1046-0.65.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.1046 - accuracy: 0.6543 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0943 - accuracy: 0.6593\n",
            "Epoch 43: loss improved from 1.10457 to 1.09435, saving model to best-model-weights-43-1.0943-0.66.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0943 - accuracy: 0.6593 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0858 - accuracy: 0.6608\n",
            "Epoch 44: loss improved from 1.09435 to 1.08583, saving model to best-model-weights-44-1.0858-0.66.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0858 - accuracy: 0.6608 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0811 - accuracy: 0.6622\n",
            "Epoch 45: loss improved from 1.08583 to 1.08106, saving model to best-model-weights-45-1.0811-0.66.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 1.0811 - accuracy: 0.6622 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0748 - accuracy: 0.6631\n",
            "Epoch 46: loss improved from 1.08106 to 1.07475, saving model to best-model-weights-46-1.0747-0.66.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0747 - accuracy: 0.6631 - lr: 0.0010\n",
            "Epoch 47/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0708 - accuracy: 0.6637\n",
            "Epoch 47: loss improved from 1.07475 to 1.07086, saving model to best-model-weights-47-1.0709-0.66.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0709 - accuracy: 0.6637 - lr: 0.0010\n",
            "Epoch 48/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0610 - accuracy: 0.6662\n",
            "Epoch 48: loss improved from 1.07086 to 1.06104, saving model to best-model-weights-48-1.0610-0.67.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0610 - accuracy: 0.6662 - lr: 0.0010\n",
            "Epoch 49/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0568 - accuracy: 0.6684\n",
            "Epoch 49: loss improved from 1.06104 to 1.05675, saving model to best-model-weights-49-1.0567-0.67.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0567 - accuracy: 0.6684 - lr: 0.0010\n",
            "Epoch 50/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 1.0478 - accuracy: 0.6699\n",
            "Epoch 50: loss improved from 1.05675 to 1.04779, saving model to best-model-weights-50-1.0478-0.67.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0478 - accuracy: 0.6699 - lr: 0.0010\n",
            "Epoch 51/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0419 - accuracy: 0.6720\n",
            "Epoch 51: loss improved from 1.04779 to 1.04188, saving model to best-model-weights-51-1.0419-0.67.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0419 - accuracy: 0.6720 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0374 - accuracy: 0.6745\n",
            "Epoch 52: loss improved from 1.04188 to 1.03739, saving model to best-model-weights-52-1.0374-0.67.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0374 - accuracy: 0.6745 - lr: 0.0010\n",
            "Epoch 53/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0301 - accuracy: 0.6746\n",
            "Epoch 53: loss improved from 1.03739 to 1.03006, saving model to best-model-weights-53-1.0301-0.67.hdf5\n",
            "1850/1850 [==============================] - 108s 59ms/step - loss: 1.0301 - accuracy: 0.6746 - lr: 0.0010\n",
            "Epoch 54/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.6778\n",
            "Epoch 54: loss improved from 1.03006 to 1.02276, saving model to best-model-weights-54-1.0228-0.68.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0228 - accuracy: 0.6778 - lr: 0.0010\n",
            "Epoch 55/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0161 - accuracy: 0.6789\n",
            "Epoch 55: loss improved from 1.02276 to 1.01608, saving model to best-model-weights-55-1.0161-0.68.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0161 - accuracy: 0.6789 - lr: 0.0010\n",
            "Epoch 56/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0081 - accuracy: 0.6821\n",
            "Epoch 56: loss improved from 1.01608 to 1.00814, saving model to best-model-weights-56-1.0081-0.68.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 1.0081 - accuracy: 0.6821 - lr: 0.0010\n",
            "Epoch 57/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 1.0053 - accuracy: 0.6836\n",
            "Epoch 57: loss improved from 1.00814 to 1.00531, saving model to best-model-weights-57-1.0053-0.68.hdf5\n",
            "1850/1850 [==============================] - 108s 59ms/step - loss: 1.0053 - accuracy: 0.6836 - lr: 0.0010\n",
            "Epoch 58/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9962 - accuracy: 0.6856\n",
            "Epoch 58: loss improved from 1.00531 to 0.99624, saving model to best-model-weights-58-0.9962-0.69.hdf5\n",
            "1850/1850 [==============================] - 108s 59ms/step - loss: 0.9962 - accuracy: 0.6856 - lr: 0.0010\n",
            "Epoch 59/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9906 - accuracy: 0.6872\n",
            "Epoch 59: loss improved from 0.99624 to 0.99065, saving model to best-model-weights-59-0.9906-0.69.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9906 - accuracy: 0.6871 - lr: 0.0010\n",
            "Epoch 60/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9902 - accuracy: 0.6876\n",
            "Epoch 60: loss improved from 0.99065 to 0.99026, saving model to best-model-weights-60-0.9903-0.69.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9903 - accuracy: 0.6875 - lr: 0.0010\n",
            "Epoch 61/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9810 - accuracy: 0.6884\n",
            "Epoch 61: loss improved from 0.99026 to 0.98102, saving model to best-model-weights-61-0.9810-0.69.hdf5\n",
            "1850/1850 [==============================] - 108s 59ms/step - loss: 0.9810 - accuracy: 0.6884 - lr: 0.0010\n",
            "Epoch 62/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9721 - accuracy: 0.6923\n",
            "Epoch 62: loss improved from 0.98102 to 0.97217, saving model to best-model-weights-62-0.9722-0.69.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9722 - accuracy: 0.6923 - lr: 0.0010\n",
            "Epoch 63/100\n",
            "1850/1850 [==============================] - ETA: 0s - loss: 0.9691 - accuracy: 0.6932\n",
            "Epoch 63: loss improved from 0.97217 to 0.96911, saving model to best-model-weights-63-0.9691-0.69.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9691 - accuracy: 0.6932 - lr: 0.0010\n",
            "Epoch 64/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9629 - accuracy: 0.6946\n",
            "Epoch 64: loss improved from 0.96911 to 0.96296, saving model to best-model-weights-64-0.9630-0.69.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9630 - accuracy: 0.6946 - lr: 0.0010\n",
            "Epoch 65/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9564 - accuracy: 0.6974\n",
            "Epoch 65: loss improved from 0.96296 to 0.95637, saving model to best-model-weights-65-0.9564-0.70.hdf5\n",
            "1850/1850 [==============================] - 108s 59ms/step - loss: 0.9564 - accuracy: 0.6974 - lr: 0.0010\n",
            "Epoch 66/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9520 - accuracy: 0.6987\n",
            "Epoch 66: loss improved from 0.95637 to 0.95206, saving model to best-model-weights-66-0.9521-0.70.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9521 - accuracy: 0.6987 - lr: 0.0010\n",
            "Epoch 67/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9438 - accuracy: 0.6999\n",
            "Epoch 67: loss improved from 0.95206 to 0.94377, saving model to best-model-weights-67-0.9438-0.70.hdf5\n",
            "1850/1850 [==============================] - 108s 59ms/step - loss: 0.9438 - accuracy: 0.6999 - lr: 0.0010\n",
            "Epoch 68/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9367 - accuracy: 0.7030\n",
            "Epoch 68: loss improved from 0.94377 to 0.93668, saving model to best-model-weights-68-0.9367-0.70.hdf5\n",
            "1850/1850 [==============================] - 108s 59ms/step - loss: 0.9367 - accuracy: 0.7030 - lr: 0.0010\n",
            "Epoch 69/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9315 - accuracy: 0.7045\n",
            "Epoch 69: loss improved from 0.93668 to 0.93155, saving model to best-model-weights-69-0.9316-0.70.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9316 - accuracy: 0.7045 - lr: 0.0010\n",
            "Epoch 70/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9234 - accuracy: 0.7075\n",
            "Epoch 70: loss improved from 0.93155 to 0.92336, saving model to best-model-weights-70-0.9234-0.71.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9234 - accuracy: 0.7075 - lr: 0.0010\n",
            "Epoch 71/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9179 - accuracy: 0.7076\n",
            "Epoch 71: loss improved from 0.92336 to 0.91794, saving model to best-model-weights-71-0.9179-0.71.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9179 - accuracy: 0.7076 - lr: 0.0010\n",
            "Epoch 72/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9142 - accuracy: 0.7087\n",
            "Epoch 72: loss improved from 0.91794 to 0.91415, saving model to best-model-weights-72-0.9142-0.71.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9142 - accuracy: 0.7087 - lr: 0.0010\n",
            "Epoch 73/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9136 - accuracy: 0.7114\n",
            "Epoch 73: loss improved from 0.91415 to 0.91360, saving model to best-model-weights-73-0.9136-0.71.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9136 - accuracy: 0.7114 - lr: 0.0010\n",
            "Epoch 74/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.9048 - accuracy: 0.7141\n",
            "Epoch 74: loss improved from 0.91360 to 0.90483, saving model to best-model-weights-74-0.9048-0.71.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.9048 - accuracy: 0.7141 - lr: 0.0010\n",
            "Epoch 75/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8992 - accuracy: 0.7150\n",
            "Epoch 75: loss improved from 0.90483 to 0.89921, saving model to best-model-weights-75-0.8992-0.72.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8992 - accuracy: 0.7150 - lr: 0.0010\n",
            "Epoch 76/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8976 - accuracy: 0.7127\n",
            "Epoch 76: loss improved from 0.89921 to 0.89763, saving model to best-model-weights-76-0.8976-0.71.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8976 - accuracy: 0.7127 - lr: 0.0010\n",
            "Epoch 77/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8931 - accuracy: 0.7159\n",
            "Epoch 77: loss improved from 0.89763 to 0.89316, saving model to best-model-weights-77-0.8932-0.72.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8932 - accuracy: 0.7158 - lr: 0.0010\n",
            "Epoch 78/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8866 - accuracy: 0.7176\n",
            "Epoch 78: loss improved from 0.89316 to 0.88664, saving model to best-model-weights-78-0.8866-0.72.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8866 - accuracy: 0.7176 - lr: 0.0010\n",
            "Epoch 79/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8790 - accuracy: 0.7205\n",
            "Epoch 79: loss improved from 0.88664 to 0.87903, saving model to best-model-weights-79-0.8790-0.72.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8790 - accuracy: 0.7205 - lr: 0.0010\n",
            "Epoch 80/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8770 - accuracy: 0.7204\n",
            "Epoch 80: loss improved from 0.87903 to 0.87701, saving model to best-model-weights-80-0.8770-0.72.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8770 - accuracy: 0.7204 - lr: 0.0010\n",
            "Epoch 81/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8671 - accuracy: 0.7231\n",
            "Epoch 81: loss improved from 0.87701 to 0.86708, saving model to best-model-weights-81-0.8671-0.72.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8671 - accuracy: 0.7231 - lr: 0.0010\n",
            "Epoch 82/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8683 - accuracy: 0.7227\n",
            "Epoch 82: loss did not improve from 0.86708\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 0.8683 - accuracy: 0.7227 - lr: 0.0010\n",
            "Epoch 83/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8625 - accuracy: 0.7254\n",
            "Epoch 83: loss improved from 0.86708 to 0.86249, saving model to best-model-weights-83-0.8625-0.73.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8625 - accuracy: 0.7254 - lr: 0.0010\n",
            "Epoch 84/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8517 - accuracy: 0.7281\n",
            "Epoch 84: loss improved from 0.86249 to 0.85169, saving model to best-model-weights-84-0.8517-0.73.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 0.8517 - accuracy: 0.7281 - lr: 0.0010\n",
            "Epoch 85/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8498 - accuracy: 0.7282\n",
            "Epoch 85: loss improved from 0.85169 to 0.84980, saving model to best-model-weights-85-0.8498-0.73.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8498 - accuracy: 0.7282 - lr: 0.0010\n",
            "Epoch 86/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8474 - accuracy: 0.7301\n",
            "Epoch 86: loss improved from 0.84980 to 0.84735, saving model to best-model-weights-86-0.8474-0.73.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8474 - accuracy: 0.7301 - lr: 0.0010\n",
            "Epoch 87/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8471 - accuracy: 0.7301\n",
            "Epoch 87: loss improved from 0.84735 to 0.84714, saving model to best-model-weights-87-0.8471-0.73.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8471 - accuracy: 0.7301 - lr: 0.0010\n",
            "Epoch 88/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8388 - accuracy: 0.7328\n",
            "Epoch 88: loss improved from 0.84714 to 0.83885, saving model to best-model-weights-88-0.8389-0.73.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8389 - accuracy: 0.7328 - lr: 0.0010\n",
            "Epoch 89/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8361 - accuracy: 0.7327\n",
            "Epoch 89: loss improved from 0.83885 to 0.83609, saving model to best-model-weights-89-0.8361-0.73.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8361 - accuracy: 0.7327 - lr: 0.0010\n",
            "Epoch 90/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8307 - accuracy: 0.7340\n",
            "Epoch 90: loss improved from 0.83609 to 0.83072, saving model to best-model-weights-90-0.8307-0.73.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 0.8307 - accuracy: 0.7340 - lr: 0.0010\n",
            "Epoch 91/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8261 - accuracy: 0.7363\n",
            "Epoch 91: loss improved from 0.83072 to 0.82610, saving model to best-model-weights-91-0.8261-0.74.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8261 - accuracy: 0.7363 - lr: 0.0010\n",
            "Epoch 92/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8203 - accuracy: 0.7375\n",
            "Epoch 92: loss improved from 0.82610 to 0.82027, saving model to best-model-weights-92-0.8203-0.74.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8203 - accuracy: 0.7375 - lr: 0.0010\n",
            "Epoch 93/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8162 - accuracy: 0.7391\n",
            "Epoch 93: loss improved from 0.82027 to 0.81621, saving model to best-model-weights-93-0.8162-0.74.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8162 - accuracy: 0.7391 - lr: 0.0010\n",
            "Epoch 94/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8151 - accuracy: 0.7392\n",
            "Epoch 94: loss improved from 0.81621 to 0.81511, saving model to best-model-weights-94-0.8151-0.74.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 0.8151 - accuracy: 0.7392 - lr: 0.0010\n",
            "Epoch 95/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8067 - accuracy: 0.7429\n",
            "Epoch 95: loss improved from 0.81511 to 0.80672, saving model to best-model-weights-95-0.8067-0.74.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 0.8067 - accuracy: 0.7429 - lr: 0.0010\n",
            "Epoch 96/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8094 - accuracy: 0.7425\n",
            "Epoch 96: loss did not improve from 0.80672\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 0.8094 - accuracy: 0.7425 - lr: 0.0010\n",
            "Epoch 97/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8024 - accuracy: 0.7425\n",
            "Epoch 97: loss improved from 0.80672 to 0.80236, saving model to best-model-weights-97-0.8024-0.74.hdf5\n",
            "1850/1850 [==============================] - 107s 58ms/step - loss: 0.8024 - accuracy: 0.7425 - lr: 0.0010\n",
            "Epoch 98/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.8006 - accuracy: 0.7449\n",
            "Epoch 98: loss improved from 0.80236 to 0.80056, saving model to best-model-weights-98-0.8006-0.74.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.8006 - accuracy: 0.7449 - lr: 0.0010\n",
            "Epoch 99/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.7966 - accuracy: 0.7461\n",
            "Epoch 99: loss improved from 0.80056 to 0.79657, saving model to best-model-weights-99-0.7966-0.75.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.7966 - accuracy: 0.7461 - lr: 0.0010\n",
            "Epoch 100/100\n",
            "1849/1850 [============================>.] - ETA: 0s - loss: 0.7908 - accuracy: 0.7463\n",
            "Epoch 100: loss improved from 0.79657 to 0.79082, saving model to best-model-weights-100-0.7908-0.75.hdf5\n",
            "1850/1850 [==============================] - 108s 58ms/step - loss: 0.7908 - accuracy: 0.7463 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79fd7053ae60>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training the larger model\n",
        "model_large.fit(X, y, epochs=100, batch_size=64, verbose=1, callbacks=desired_callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H2AgMUnfhuq"
      },
      "outputs": [],
      "source": [
        "model_large.save('model_large.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YmmzZq880OSj"
      },
      "outputs": [],
      "source": [
        "# Loading the model weights\n",
        "\n",
        "# model_large.load_weights('model_large.h5')\n",
        "filename = \"best-model-weights-100-0.7908-0.75.hdf5\" \n",
        "model_large.load_weights(filename)\n",
        "model_large.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZiNITMb0OSk",
        "outputId": "a8025841-bf4a-4993-836b-17fdb6ea08ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" leman, “would the young lady not like to\n",
            "come in and play for us here i \"\n",
            "Generated text:\n",
            "\n",
            "n the floor. He would not be tomened to be someone from his father and sister to think about the chief clerk them with the strength and then had been slightly unwell then, and without decisively or at the contraight away, as he was still have to be some of them and still because he was so much more some of that he was still have to be the same time. They had to be seen in a chair and some super family then had been suddenly come in that he was something while his sister was something that he was something that he would have been struck in the flat and slightest happening, he was something with\n"
          ]
        }
      ],
      "source": [
        "# Generating some text from random seed text\n",
        "\n",
        "generated_text = generate_text(num_chars=600, lstm_model=model_large)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OHk_95C0OSk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
