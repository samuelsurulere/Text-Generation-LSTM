# Text-Generation-LSTM
Text generation using the LSTM algorithm in the TensorFlow library.

## Project aims/goals

The field of NLP has attracted a lot of interest in recent years. Due to this attention, several modern applications of NLP can be found in constantly increasing areas of businesses and organizations. In this project, I delve into NLP by training an LSTM model to generate text using the [TensorFlow](https://www.tensorflow.org/) library. The dataset used for training the ML model is the Metamorphosis book authored by Franz Kafka. The dataset can be downloaded from the [Project Gutenberg](https://www.gutenberg.org/cache/epub/5200/pg5200.txt) website. Some metadata information before and after the book text were deleted from the dataset. 

## Data Preprocessing

No preprocessing techniques (such as text cleaning or formatting) were applied to the dataset. The corpus was prepared for input to the model by creating word to vectors (word2vec) for the training and inference steps. The maximum length was assigned as the length of the longest sentence in the text. The training data was padded to ensure that all the training vectors had the same length. The labels (targets) were also converted to vectors (categorical format). The data is now set and prepared to be passed as input into the model for training.

## Model selection and evaluation

Two LSTM models were used to train on the processed dataset. The first was a small model with only two LSTM layers (256 and 128 units) and a Dense layer, while the larger model had three LSTM layers (1024, 128, 128 units), three Dropout layers (0.2) and one Dense layer. The smaller model trained for 100 epochs over two hours on a 4GB NVIDIA QUADRO 1200 GPU. The model had an accuracy of 89%. To train the larger model for the same number of epochs, I used the 16GB T4 GPU on Google Colab which took almost an hour. Training on my laptop would probably take more than 16 hours to complete. The larger model had a lower accuracy of 75%. The loss of the smaller model was also less than that of the larger model. I do not have an explanation for this, as my expectation was that the larger model will have higher accuracy and lower loss. I also experienced this phenomenon while training the models on another dataset.

### Key takeaways

Since it was quite difficult to differentiate between the predictions generated by the models, the small model will be preferred. The following reasons are the motivation for choosing the smaller model:
 - It has a smaller size, 
 - Takes a shorter time to train, 
 - Has a higher accuracy and a lower loss.

### Repo structure

```
├── README.md           <-- Markdown file explaining the project's 
|                              approach, and findings
│
├── metamorphosis_clean.txt       <-- Dataset used in preprocessing and
|                                           training the LSTM models.
│
├── text_generation.ipynb   <-- File containing the code to obtain the
|                                 project's results.
│
├── best-model-weights-100-0.7908-0.75.hdf5    <-- Saved model with some 
|                                                    info.
|                              
├── model_large.h5        <-- Saved LSTM model.
├── model_small.h5        <-- Saved LSTM model (small).
```

The inspiration for this project came after I completed the NLP - Natural Language Processing with Python on Udemy.